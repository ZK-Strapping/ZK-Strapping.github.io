---
title: BGV Scheme (1)
date: 2024-01-16 17:00:00 +0900
categories: [HE, FHE, BGV]
tags: [HE, FHE, BGV]
pin: false
math: true
mermaid: true
image:
  path: /assets/img/tokyo1.jpg
  alt: My Tokyo photo
---

## A Candidate Scheme
1980년에 나온 이 scheme은 (semantically) scure하지 않으나, 이어서 나오는 LWE(Learning With Errors)와 깊은 관련이 있습니다. 한 번 알아보도록 합시다.

이 scheme의 setup 과정은 다음과 같습니다. 
> $2$개의 parameter, $n$과 $q$를 정합니다. $n$은 key의 dimension을 뜻하고, q는 modulus를 뜻합니다. 이전 글에서 BGV Scheme은 Finite Field $\mathbb{Z}_q$에서 동작한다고 언급했는데, 이 때 $q$와 동일한 notation입니다. 

이후, key generation 과정을 거칩니다.
> Secret key $\mathbf{s}=(s_1, s_2, \cdots, s_n) \in \mathbb{Z}^n$이며, 이 때 $s$는 dimension $n$의 벡터가 됩니다. 

두 과정 이후, Encryption과 Decryption 과정을 다음과 같이 정의합니다.
> ***Encryption***  
암호화하고자 하는 메시지 $m\in \mathbb{Z}$이 우선 존재합니다. Sample이라고 하는 크기 $n$의 벡터 $\mathbf{a}$를 다음과 같이 생성합니다: $\mathbf{a}=(a_1, a_2, \cdots, a_n)\leftarrow U(\mathbb{Z}^n_q).$ 이는, $\mathbb{Z}_q$에서 $n$개의 수를 독립적으로 random하게 뽑아 $a_1, \cdots, a_n$에 할당하는 것을 의미한다고 말할 수 있습니다. 이 때, $a_0:=-(a_1s_1+\cdots+a_ns_n)+m \mod{q}$를 정의하며 계산합니다. 메시지 $m$을 암호화한 결과는 크기 $m+1$의 벡터, $\mathbf{c}=(a_0, a_1, \cdots, a_n)\in \mathbb{Z}_q^{n+1}$이 됩니다.

> ***Decryption***  
암호화된 결과 $\mathbf{c}=(a_0, a_1, \cdots, a_n)$에 대해 Decryption을 시도하는 user가 알고 있는 secret key $\mathbf{s}=(s_1, \cdots, s_n)$에 대하여, Encryption 과정에서 $a_0$의 정의로부터 $m=a_0+a_1s_1+\cdots+a_ns_n \mod{q}$가 됩니다.

과연 이 scheme은 legit할까요? scheme이 legit함은, $Dec(Enc(m))=m$이 됨을 의미합니다. 잘 생각해보면, 특정 조건이 만족된다면 이는 legit함을 관찰할 수 있습니다. 그 특정 조건은, $\lvert m\rvert<\frac{1}{2}q$란 사실을 생각해볼 수 있습니다. 가능한 $m$의 구간의 길이가 $q$ 미만이어야 modular 연산에 의한 값의 달라짐이 발생하지 않습니다.

한편, 마지막으로 한 번 생각해 봅시다. 이 scheme은 (semantically) secure할까요? 우선 chosen-plaintext attack과 같은 경우에는, 이 scheme은 취약하게 됩니다. 만약 $n$개 이상의 message가 $0$인 plaintext-ciphertext 쌍이 주어졌다고 가정해 봅시다. $i$번째 ciphertext $c_i=(a_{i,0}, a_{i,1}, \cdots, a_{i, n})$에 대해 $a_{i,0}+a_{i,1}s_1, \cdots, a_{i, n}s_n\equiv 0 \mod{q}$가 됩니다. 이러한 식이 성립하므로, 다음과 같은 식을 통해 secret key의 벡터 $\mathbf{s}$를 구할 수 있습니다.

$$\begin{pmatrix} a_{1, 0} \\ a_{2, 0} \\ \vdots \\ a_{n, 0}\end{pmatrix}+\begin{pmatrix} a_{1, 1} & a_{1, 2} & \cdots & a_{1, n} \\ a_{2, 1} & a_{2, 2} & \cdots & a_{2, n} \\ \vdots \\ a_{n, 1} & a_{n, 2} & \cdots & a_{n, n} \end{pmatrix}\begin{pmatrix} s_1 \\ s_2 \\ \vdots \\ s_n \end{pmatrix} = \begin{pmatrix}0 \\ 0 \\ \vdots \\ 0\end{pmatrix}$$

정확히 어떻게 구할 수 있을까요?​ 임의의 random matrix가 invertible일 확률은 $1$에 수렴하므로, 위의 $a_{1, 1}$부터 $a_{n, n}$까지 표기된 행렬의 역행렬을 구한 뒤 양변에 곱하면 secret key를 얻을 수 있고, 따라서 위 scheme은 secure하지 않습니다.

## Learning with Errors

LWE Problem은 Oded Regev에 의해 2005년 제기된 computational problem입니다. LWE Problem에서 사용하는 scheme은 A Candidate Scheme의 그것과 매우 유사한데, 한 가지 차이점만 존재합니다.

LWE distribution은 $$\mathrm{LWE}_{n, q, \sigma}(\mathbf{s})$$ 로 표기하며, $3$개의 parameter $n, q, \sigma$로 이루어져 있습니다. $n$은 dimension, $q$는 modulus로 앞에서와 동일하지만, $\sigma$가 새로 추가되었습니다. 이는 표준편차를 나타내는데, 이는 표준편차 $\sigma$인 discrete gaussian distribution $D_\sigma$에서 정수 $e$를 추출하는 데 쓰입니다. 이 때, discrete gaussian은 다음과 같은 형태의 확률분포를 의미합니다.

![discrete_gaussian](/assets/img/discrete_gaussian.png)

LWE instance는 우선 input으로 secret vector $\mathbf{s}$를 가집니다. 또한, 전에 다뤘던 scheme과 동일하게 $$\mathbb{Z}_q$$에서 $n$개의 수를 random하게 추출해 sample $a_1, \cdots, a_n$을 정합니다. 또한, 이전에 설명한 대로 discrete gaussian $D_\sigma$에서 임의의 정수 $e$를 추출합니다. 이후, $a_0=-(a_1s_1+\cdots+a_ns_n)+e\mod{q}$를 정하고, $(a_0, a_1, \cdots, a_n)\in \mathbb{Z}_q^{n+1}$을 반환합니다.

이제 LWE Problem은, 과연 아까처럼 적당히 원하는 plaintext-ciphertext pair가 주어지면 secret key를 찾을 수 있는지의 유무입니다. 이는 현재까지 굉장히 어렵다고 알려져 있으며, quantum computing으로도 저격하기 힘든 것으로 알려져 있습니다. 괜찮은 아이디어가 생각나신다면 블로그의 이메일로 연락 주세요.

이제 여기서, 두 가지 질문이 있습니다. 하나는 유명한 질문이고, 두 번째는 필자도 모르는 open question입니다.

LWE assumption은 두 가지 버전이 있습니다.
- Computational version: 아까 이야기한 내용과 동일합니다. 많은 $\mathrm{LWE}_{n,q,\sigma}(\mathbf{s})$의 sample이 존재했을 때 $\mathbf{s}$를 찾기 어려운지의 유무입니다.
- Decisional version: 두 distribution $\mathrm{LWE}_{n,q,\sigma}(\mathbf{s})$와 $U(\mathbb{Z}_q^{n+1})$이 구별 불가능한지의 유무에 대한 내용입니다.

상식적으로 Decisional version을 풀면 Computational version은 자명해 보입니다. 하지만 과연 역은 성립할까요? 성립합니다. 이를 증명할 수 있으며, 자세한 내용은 생략합니다. 즉, 두 assumption은 equivalent한 관계라는 사실을 알 수 있습니다.

한편, 저도 모르는 질문 하나를 남깁니다. 왜 여기서 gaussian distribution을 사용했을까요? 그 이유가 궁금합니다.

## BGV Scheme (Before Linearization)
